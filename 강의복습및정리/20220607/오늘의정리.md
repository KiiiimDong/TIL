# 오늘의 정리(어텐션 , 트랜스포머)

## 1. seq2seq의 한계와 어텐션의 등장 및 어텐션의 개념.

1. seq2seq(Sequence to sequence)의 한계와 어텐션(attention)의 등장
- seq2seq  모델은 인코더에서 입력 시퀀스를 컨텍스트 벡터라는 하나의 고정된 크기의 벡터 표현으로 압축하고 디코더는 이 컨텍스트 벡터를 통해서 출력 시퀀스를 만들어 낸다.
- 이런 방식의 RNN기반의 seq2seq 모델은 두가지 큰 문제가 있다.
    - 첫째로 하나의 고정된 크기의 벡터를 설정하다 보니 정보를 그 벡터의 크기에 맞춰 압축하면서 정보 손실이 생길 수 있다는 점이다.
    - 두번째로 RNN기반의 고질적인 문제인 기울기 소실문제의 발생으로 입력문장이 길면 번역품질이 떨어지는 현상이 나타난다.
- 위의 문제점들을 보정하기 위해 등장한 개념이 어텐션이라는 기법이다.

2. 어텐션의 아이디어
- 어텐션의 기본 개념은 디코더에서 출력단어를 예측하는 매 시점마다, 인코더에서 전체 입력문장을 다시 한 번 참고하는 것이다.
- 이때, 전체 입력 문장을 전부 동일한 비율로 참고하는 것이 아니라, 해당 시점에서 예측해야할 단어와 관련 있는 입력 단어 부분을 좀 더 집중해서 본다는 것이 핵심개념이다.

3. Key-Value 자료형 
- 파이썬의 딕셔너리와 같은 key와 value가 대응 되는 개념을 이용한다.

4. 어텐션 함수(Q, K, V)
- attention(Q, K, V) : attention value
- 주어진 쿼리(Query)에 대해서 모든 키(key)와의 유사도를 각각 구하여 키와 맵핑되어있는 각각의 값(value)에 반영하고, 그 value를 모두 더해서 리턴하는 것이 어텐션 값이다.
- Q = Query : t 시점의 디코더 셀에서의 은닉상태
- K = Keys : 모든 시점의 안코더 셀의 은닉상태들
- V = Values : 모든 시점의 인코더 셀의 은닉상태들

## 2. 어텐션의 메커니즘과 종류

1. 어텐션의 메커니즘
- 어텐션의 종류에 따라 메커니즘이 조금씩 달라질 수 있는데, 이해하기 쉽게 수식을 적용한 닷-프로덕트어텐션을 기반으로 개념을 정리한다.
- 어텐션의 메커니즘의 작동 순서
    1. 어텐션 스코어를 구한다.
        - 어텐션 스코어란 현재 디코더의 시점 t에서 단어를 예측하기 위해, 인코더의 모든 은닉 상태 각각이 디코더의 현 시점의 은닉 상태(ŝt) 와 얼마나 유사한지를 판단하는 스코어값이다.
    2. 소프트맥스함수를 통해 어텐션 분포를 구한다.
    3. 각 인코더의 어텐션 가중치와 은닉상태를 가중합하여 어텐션 값을 구한다.
    4. 어텐션 값과 디코더의 t시점의 은닉상태를 연결한다.
    5. 출력층 연산의 입력이 되는 ŝt를 계산한다.
    6. ŝt를 출력층의 입력으로 사용한다.

2. 어텐션 종류(참고만)
- seq2seq+어텐션 모델에 쓰일 수 있는 다양한 어텐션 종류가 있지만, 닷-프로덕트 어텐션을 기반으로 이해한 개념을 기반으로 생각하면 된다. 다른 어텐션과 중간 수식이 다를 수 있지만, 기본적인 개념은 크게 바뀌지 않는다.
- dot, scaled dot, general, concat, location-base 등이 있다.

3. 바다나우 어텐션(Bahdanau)(실습코드로 학습)
- 닷 프로덕트 어텐션 => 루옹 어텐션
- concat 어텐션 => 바다나우 어텐션
- 수식을 일일이 외우고 있을 필요는 없고, 실습코드를 통해 이해하는 선에서 학습한다.

## 3. 트랜스포머(transformer)

1. 트랜스포머의 개념
- 트랜스포머는 RNN을 사용하지 않고 단순히 어텐션으로 인코더-디코더 구조를 설계하였음에도 번역 성능에서도 RNN보다 우수한 성능을 보여준다.
- 어텐션을 RNN의 보정을 위한 용도로서 사용하는 것이 아니라 어텐션만으로 인코더와 디코더를 만든것이다.
- RNN을 사용하지 않지만 인코더-디코더 구조를 유지하고 있다. 
- 이전 seq2seq 구조에서는 인코더와 디코더에서 각각 하나의 RNN이 t개의 시점(time step)을 가지는 구조였다면 이번에는 인코더와 디코더라는 단위가 N개로 구성되는 구조다. 

2. 트랜스포머의 주요 하이퍼파라미터
- dmodel = 512 : 트랜스포머의 인코더와 디코더에서의 정해진 입력과 출력의 크기를 의미한다. 
- num_layers = 6: 트랜스포머에서 하나의 인코더와 디코더를 층으로 생각하였을 때, 트랜스포머 모델에서 인코더와 디코더가 총 몇 층으로 구성 되었는지를 의미한다. 
- num_heads = 8 : 트랜스포머에서는 어텐션을 사용할 때, 한 번 하는 것 보다 여러 개로 분할해서 병렬로 어텐션을 수행하고 결과값을 다시 하나로 합치는 방식을 택했다. 이때 이 병렬의 개수를 의미한다.
- dff = 2048 : 트랜스포머 내부에는 피드 포워드 신경망이 존재하며 해당 신경망의 은닉층의 크기를 의미한다. 

## 4. 포지셔널 인코딩(positional encoding)

1. 포지셔널 인코딩의 기본개념
- RNN은 단어를 순차적으로 입력받아서 처리하여 각 단어의 위치정보를 가질 수 있지만, 트랜스포머는 단어 입력을 순차적으로 받는 방식이 아니라서 위치정보를 다른 방식으로 기억할 필요가 있다.
- 단어의 위치정보를 얻기 위해서 각 단어의 임베딩 벡터에 위치 정보들을 더하여 모델의 입력으로 사용하는 것을 포지셔널 인코딩이라고 한다.
- 입력으로 사용되는 임베딩 벡터들이 트랜스포머의 입력으로 사용되기 전에 포지셔널 인코딩의 값이 더해진다.

2. 포지셔널 인코딩의 수식
- 사인함수와 코사인 함수의 값을 임베딩 벡터에 더해줌으로써 단어의 순서정보를 더하여 준다.
- 구체적인 수식은 외운다기 보다는 사인 코사인을 이용한 원리 정도만 알고 있자.
- 이러한 원리를 이용한 포지셔널 인코딩 결과 트랜스포머의 입력은 순서정보가 고려된 임베딩 벡터가 된다. 이는 각 단어의 위치정보를 가질 수 있게 됨을 뜻한다.
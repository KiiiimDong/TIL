# 오늘의 정리
## 1. 경사하강법(gradient descent)
1. 경사하강법의 개념
- 오차 그래프에서 오차가 가장 작은 방향으로 이동시키는 방법. 
- 오차가 가장 작은 최솟값을 찾기 위해서 그래프에서 미분을 활용해서 기울기가 0이되는 지점을 찾아나가는 방식이다.
- 반복적으로 기울기를 변화시키면서 m의 값을 찾아내는 것이 경사하강법이다.
- 이 때 기울기를 변화시킬 때 변화하는 정도를 학습률이라고 한다.
2. 학습률
- 대표적인 하이퍼파라미터 중 하나다.
    - 하이퍼 파라미터 : 사용자가 직접 값을 정해줘야 하는 파라미터, 튜닝이 필요하다.
- 항상 좋은 값이라는게 존재하지 않고, 상황에 맞게 적절하게 값을 바꿔가면서 최적의 학습률을 찾아나가는 과정이 필요하다.    
## 2. 오차역전파(back-propagation)
- 경사하강법을 확장한 개념.
- 임의의 가중치를 선언하고 결과값을 구한 다음 실제 값과 비교해 오차를 구한 다음 오차를 토대로 앞선 가중치를 차례로 거슬러 올라가면서 갱신해나가는 과정.
- 오차가 작아지는 방향으로 계속해서 가중치를 업데이트하는 과정이다.
- 딥러닝에서 신경망 내부의 가중치를 수정해나가는 기본적인 메커니즘이다.
## 3. 딥러닝 모델의 기초
- 모델 예시
```python
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Dense # 제일 기본적인 층
model = Sequential()
model.add(Dense(44, input_dim=22, activation='relu'))  # 입력층 노드 22개
                                                       # 은닉층1 노드 44개
model.add(Dense(1, activation='sigmoid'))              # 출력층 노드 1개
# 딥러닝을 실행
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
# 모델 학습
model.fit(X, Y, epochs=100, batch_size=10) 
```
- 모델을 Sequential로 만들어주고 시작, 입력층과 출력층의 개수는 데이터에 맞게 설정한다.
- 이진분류일때는 loss='binary_crossentropy', 다중분류 일때는 loss = 'categorical_crossentropy'
- 이진분류일때는 출력층의 activation ='sigmoid', 다중분류 일때는 activation = 'softmax'
- optimizer는 다양한 종류가 있는데, 모르겠으면 adam
- metrics는 상황에 맞는걸로 쓰면된다.
- batch_size는 데이터가 한 번에 들어가는 크기, 항상 좋은 값은 없고 최적의 값을 찾아나가야 한다.

## 4. 오차함수(Loss function)
- 회귀문제에 사용하는 것들(속도가 느리다는 단점이 있다.)
    - MSE : 평균제곱오차
    - MAE : 평균절대오차
    - MSE : 평균절대백분율오차
    - MSE : 평균제곱로그오차
- 분류문제에 사용하는것들 (이진, 다중)
    - 교차엔트로피 계열이라고도 한다.
    - categorical_crossentropy : 다중분류
    - binary_crossentropy : 이진분류
    
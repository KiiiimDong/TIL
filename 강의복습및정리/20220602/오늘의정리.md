# 오늘의 정리
## 1. 워드임베딩(Word Embedding)
1. 희소표현(원핫벡터) vs 밀집표현(임베딩벡터)
- 희소표현의 한계: 희소 벡터는 단어의 개수가 늘어나면 벡터의 차원이 한없이 커진다.-> 공간적 낭비를 일으킨다.
- 밀집표현으로 희소표현의 한계 극복: 희소표현과 반대되는 표현인 밀집표현, 밀집표현은 벡터의 차원을 단어의 집합크기로 설정하지 않고, 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춘다. -> 차원수는 줄이고 0대신 실수값들이 들어가면서 조밀해진다.
- 임베딩의 개념: 단어를 밀집 벡터(임베딩벡터라고도 한다.) 형태로 표현하는 방법.
- 임베딩의 방법론: LSA,Word2Vec,FastText,Glove ...
- keras의 도구인 Embedding() : 단어를 랜덤한 값을 가지는 밀집 벡터로 변환하고 인공 신경망의 가중치를 학습하는 것과 같은 방식으로 단어 벡터를 학습한다.

## 2. 워드투벡터(Word2Vec)
- 워드투벡터의 개념 : 단어벡터간 유사도를 반영하는 방법
- 희소표현 vs 분산표현 : 표현하고자 하는 단어의 인덱스값만 1이고, 나머지는 0인 원-핫벡터의 표현방식을 희소 표현이라 한다. 이와 달리 단어의 의미를 다차원 공간에 벡터화하는 것을 분산표현, 그리고 분산표현을 이용하여 단어 간 의미적인 유사성을 벡터화 하는 것을 워드 임베딩이라고 한다. 
- 워드투벡터의 두가지 방법 : CBOW, Skip_Gram
- CBOW(Continuous Bag of Words) : 주변의 단어로 중심단어 예측한다.
    - window : 중심단어를 예측하기 위해서 앞,뒤로 몇개의 단어를 보는지 설정하는 범위.
    - sliding window: 윈도우 크기가 정해지고나서 윈도우를 옆으로 움직여서 주변 단어와 중심 단어의 선택을 변경해가며 학습을 위한 데이터 셋을 만드는 것. 
    - 입력층 : 앞,뒤로 사용자가 정한 윈도우 범위안에 있는 주변의 원-핫벡터가 들어간다.
    - 출력층 : 예측하고자하는 중간 단어의 원-핫 벡터가 레이블로서 필요하다.
    - 은닉층 : Word2Vec은 은닉층이 다수인 딥 러닝 모델이 아니라 은닉층이 1개인 얕은 신경망이다. 다른 일반적인 은닉층과 다르게 활성화 함수가 존재하지 않고, 룩업 테이블이라는 연산을 담당하는 층으로 투사층이라고도 한다.
- Skip_Gram : 중심단어에서 주변단어를 예측, 일반적으로 CBOW보다 더 성능이 좋다.

## 3. NNLM(피드 포워드 신경망 언어모델) vs Word2Vec
- NNLM의 속도와 정확도 개선 -> Word2Vec
- NNLM 쓸 일 거의 없음
- NNLM : 이전단어를 참고하여 다음 단어를 예측하는 언어 모델이다.
- Word2Vec : 워드 임베딩이 목적이다. 전,후 단어를 모두 참고하여 중심단어를 예측한다. 은닉층을 제거해 속도가 더 빠르다.
## 4. 케라스를 이용한 텍스트 분류(코드로 공부)
- 스팸메일 분류
- 로이터 뉴스 분류
- 네이버영화감성분석
# 오늘의 정리(BERT)

## 1. NLP에서의 사전 훈련(Pre-training)
1. 사전 훈련된 워드 임베딩
- 사전 훈련된 워드 임베딩을 사용하면 기존의 Word2Vec, FastText,GloVe가 가지고 있던 하나의 단어가 하나의 벡터값으로 맵핑되어 문맥을 고려하지 못 하여 다의어나 동음이의어를 구분하지 못하는 문제점을 해결할 수 있다. 이러한 방식을 쓰는 것 중 BERT를 중점적으로 다뤄볼것이다.

2. 사전 훈련된 언어 모델
- 사전 훈련된 언어 모델의 강점은 학습 전 사람이 별도 레이블을 지정해줄 필요가 없다는 점이다.
-  NLP의 주요 트렌드는 사전 훈련된 언어 모델을 만들고 이를 특정 태스크에 추가 학습시켜 해당 태스크에서 높은 성능을 얻는 것으로 접어들었고, 언어 모델의 학습 방법에 변화를 주는 모델들이 등장했다. 

3. 마스크드 언어 모델(Masked Language Model)
- 마스크드 언어 모델은 입력 텍스트의 단어 집합의 15%의 단어를 랜덤으로 마스킹(Masking)한다. 여기서 마스킹이란 원래의 단어가 무엇이었는지 모르게 한다는 것이다. 그리고 인공 신경망에게 이렇게 마스킹 된 단어들을 예측하도록 한다. 

## 2. BERT(Bidirectional Encoder Representations from Transformers)
1. BERT의 개요
- 트랜스포머를 이용하여 구현되었으며, 위키피디아(25억 단어)와 BooksCorpus(8억 단어)와 같은 레이블이 없는 텍스트 데이터로 사전 훈련된 언어 모델입니다.
- 다른 작업에 대해서 파라미터 재조정을 위한 추가 훈련 과정을 파인 튜닝(Fine-tuning)이라고 합니다.

2. BERT의 크기
- BERT의 기본 구조는 트랜스포머의 인코더를 쌓아올린 구조입니다. Base 버전에서는 총 12개를 쌓았으며, Large 버전에서는 총 24개를 쌓았습니다. 그 외에도 Large 버전은 Base 버전보다 d_model의 크기나 셀프 어텐션 헤드(Self Attention Heads)의 수가 더 큽니다. 트랜스포머 인코더 층의 수를 L, d_model의 크기를 D, 셀프 어텐션 헤드의 수를 A라고 하였을 때 각각의 크기는 다음과 같습니다.
    - BERT-Base : L=12, D=768, A=12 : 110M개의 파라미터
    - BERT-Large : L=24, D=1024, A=16 : 340M개의 파라미터
    
3. BERT의 문맥을 반영한 임베딩(Contextual Embedding)

4. BERT의 서브워드 토크나이저 : WordPiece

5. 포지션 임베딩(Position Embedding)

6. BERT의 사전 훈련(Pre-training)

7. 세그먼트 임베딩(Segment Embedding)

8. BERT를 파인 튜닝(Fine-tuning)하기
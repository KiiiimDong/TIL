# 오늘의 정리
## 1. 언어모델(Language Model, LM)
1. 언어모델(LM)
    - 언어 모델은 언어라는 현상을 모델링하고자 단어 시퀀스(문장)에 확률을 할당하는 모델이다.
    - 언어 모델을 만드는 방법은 통계를 이용한 방법과 인공 신경망을 이용한 방법으로 구분할 수 있다.
    - 최근에는 통계를 이용한 방법보다는 인공 신경망을 이용한 방법이 더 성능이 좋기 때문에 인공 신경망 방법 위주로 연구되고 있다.
    - 단어 시퀀스에 확률을 할당하는 가장 보편적인 방법은 언어 모델이 이전 단어들이 주어졌을 때 다음 단어를 예측하도록 하는 것이다. 
2. 통계적 언어모델(Statistical Language Model, SLM)
    - 카운트 기반의 접근이 기본개념이다.
    - 카운트 기반접근의 한계때문에 희소 문제(sparsity problem)이 발생하는데 이 문제는 훈련한 코퍼스에 단어가 없을 경우에 확률을 계산할 수 없어서 현실적인 확률과 거리가 있다는 것이다.
    - 희소 문제를 완화하기위해 N-gram이나 다른 일반화기법들을 사용해도 희소 문제에 대한 근본적인 해결책이 되지는 못하고, 이를 극복하는 방법으로 인공 신경망 방법을 이용할 것이다.
3. N-그램 언어모델(N-gram Language Model)
    - 이전에 등장한 모든 단어를 고려하는 것이 아니라 일부 단어(n개)만 고려하는 접근 방법을 사용한 방법이다.
    - 희소 문제를 완화한 방법이긴 하지만 여전히 근본적인 해결책을 제시하지 못해서 희소 문제를 벗어날 수 없다.
    - n을 결정하는것은 정확도와 희소 문제에 있어서 trade-off 관계를 갖는다. n을 늘리면 정확도가 올라가지만, 그만큼 희소 문제가 커진다.
4. 펄플렉서티(Perplexity, PPL)
    - 모델 내에서 자신의 성능을 수치화하여 결과를 내놓는 것인데, 헷갈리는 정도를 나타내는 것이다.
    - PPL은 선택할 수 있는 가능한 경우의 수를 의미하는 분기계수(branching factor)이다.
    - 수치가 낮을수록 좋은성능(덜 헷갈리는 것), 수치가 높을수록 안좋은성능(더 헷갈리는 것) 
## 2. 카운트 기반의 단어표현(Count based word Representation)
1. 국소표현과 분산표현
    - 국소표현은 그 단어만 보고 그 단어를 표현하는 것이고, 분산표현은 연관성있는 주변을 같이 보고 단어를 표현하는 것이다.
2. Bag of Words(BoW)
    - 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도수에만 집중하는 텍스트 데이터의 수치화 표현 방법이다.
    - CountVectorizer 클래스를 이용하면 BoW를 쉽게 만들 수 있고, 불용어 처리를 하면 불용어가 없어진 BoW를 만들 수 있다.
3. DTM(Document-Term Matrix)
    - BoW 표현을 다수의 문서에 대해서 행렬로 표현하고 부르는 것이다.
    - 희소 문제를 갖는다는 점과, 단순 빈도수기반 접근이라는 한계점이 있다.
4. TF-IDF
    - TF와 IDF를 고려한 값인데, 쉽게 말하면 빈도수와 중요도를 같이 고려한 지표이다.
    - TF는 특정 문서 d에서의 특정 단어 t의 등장 횟수이고,  df는 특정 단어 t가 등장한 문서의 수이다. IDF는 df에 반비례하는 수이다.
    - TfidfVectorizer클래스를 이용하면 쉽게 사용할 수 있다.
    - 개념적으로 단순 빈도수 기반이 아닌 문서에서의 중요도를 반영하는 값이라는 것이 핵심이다.
## 3. 벡터의유사도(Vector Similarity) 
1. 코사인유사도(Cosine Similarity)
    - 가장 많이 사용하는 유사도.
    - 추천시스템 : CBF와 CF가 있다. genre CBF가 성능이 좋을 때가 있다. 
2. 유클리드 거리
3. 자카드 유사도

## 4. 벡터와 행렬연산
1. 텐서(tensor, 통상적으로 3차원이상부터 텐서라고 부른다.)
    - 0차원

    - 1차원

    - 2차원

    - 3차원
2. 내적(dot product)
    - 
## 5. 피드 포워드 신경망(NNLM)
1. 
## 6. 순환 신경망(RNN)
1. RNN
    - 
2. LSTM(제일많이쓰는모델)
    -
3. GRU
    - LSTM과 차이점만 필기하기.
4. RNN언어모델(RNNLM)
    -
5. teacher~
 